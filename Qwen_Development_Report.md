# Qwen系列大语言模型发展史与技术报告

## 目录

1. [概述](#概述)
2. [Qwen1系列：起点](#qwen1系列起点)
3. [Qwen1.5系列：规模化扩展](#qwen15系列规模化扩展)
4. [Qwen2系列：全面升级](#qwen2系列全面升级)
5. [Qwen2.5系列：多样化与优化](#qwen25系列多样化与优化)
6. [Qwen3系列：最新突破](#qwen3系列最新突破)
7. [Qwen3 8B 详细架构分析](#qwen3-8b-详细架构分析)
8. [技术演进总结](#技术演进总结)
9. [参考文献](#参考文献)
10. **[附录1：专业术语解释](#附录1专业术语解释)**
11. **[附录2：Qwen3-8B 快速使用](#附录2qwen3-8b-快速使用)**

---

## 概述

Qwen（通义千问）是阿里巴巴推出的大语言模型系列。自2023年发布以来，Qwen系列经历了快速迭代，从最初的对话模型发展到涵盖0.6B到235B参数的完整模型矩阵，成为全球最具影响力的大模型开源项目之一。

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          Qwen系列发展时间线                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  2023     Qwen1.0      Qwen-7B/14B/70B                                     │
│    │        │                                                                  │
│    │        └─────────────────────────────────────┐                           │
│    │                                              ▼                           │
│  2024.02   Qwen1.5        首个MoE模型 Qwen1.5-MoE-A2.7B                     │
│    │        │            (2.7B激活参数，14.3B总参数量)                          │
│    │        │                                                                  │
│    │        ├───────────────────────────┬─────────────────────────────────┐   │
│    │        ▼                           ▼                                   │
│  2024.06  Qwen2发布                2024.09  Qwen2.5发布                    │
│           多语言增强                  新增3B/14B/32B尺寸                       │
│                                                                             │
│    │        └───────────────────────────────────────────────────┐           │
│    │                                                              ▼           │
│  2025.04                                                       Qwen3发布    │
│            235B-MoE/30B-MoE/8B/4B/0.6B  思考模式切换（比如gemini flash和pro的区别）  100+语言支持            │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Qwen1系列：起点

### 1.1 发布背景

Qwen1系列（Qwen-7B/14B/70B）是阿里巴巴于2023年推出的大语言模型系列，作为Qwen系列的奠基版本。

### 1.2 技术特点

| 模型 | 参数规模 | 上下文长度 | 特点 |
|------|---------|-----------|------|
| Qwen-7B | 7B | 64K | 首个开源版本，高性价比 |
| Qwen-14B | 14B | 64K | 更强性能，适合复杂任务 |
| Qwen-70B | 70B | 64K | 对标GPT-3.5级别 |

### 1.3 核心贡献

- 建立中文大模型开源标杆
- 完善训练数据集（高质量中英文混合）
- 推出配套的量化部署工具

---

## Qwen1.5系列：规模化扩展

### 2.1 系列成员

Qwen1.5系列包含多个尺寸的稠密模型和**首个MoE模型**：

| 模型 | 类型 | 激活参数 | 总参数量 |
|------|------|---------|---------|
| Qwen1.5-0.5B | 稠密 | 0.5B | 0.5B |
| Qwen1.5-1.8B | 稠密 | 1.8B | 1.8B |
| Qwen1.5-4B | 稠密 | 4B | 4B |
| Qwen1.5-7B | 稠密 | 7B | 7B |
| Qwen1.5-14B | 稠密 | 14B | 14B |
| Qwen1.5-32B | 稠密 | 32B | 32B |
| **Qwen1.5-MoE-A2.7B** | **MoE** | **2.7B** | **14.3B** |

### 2.2 Qwen1.5-MoE-A2.7B 核心技术

#### 2.2.1 架构设计

```
┌─────────────────────────────────────────────────────────────────┐
│                  Qwen1.5-MoE 架构                                │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   输入 ──┬── 共享专家 × 4 ──┬── 融合输出                          │
│          │                  │                                    │
│          └── 路由专家 × 60 ─┘                                    │
│                 │                                                  │
│            Top-2 路由选择                                        │
│            (每次激活2个专家)                                      │
│                                                                 │
│   特点：                                                         │
│   • 64位细粒度专家（是传统MoE的8倍）                             │
│   • 4个共享专家（始终激活）                                       │
│   • 60个路由专家（按需激活）                                      │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

#### 2.2.2 Upcycling 训练策略

Qwen1.5-MoE **并非从零开始训练**，而是采用**upcycling**策略：

```
Qwen-1.8B (稠密模型)
        │
        ▼
   专家参数复制
        │
        ▼
   添加随机初始化
        │
        ▼
   MoE预训练
```

**优势**：
- 训练成本降低 **75%**（相比从头训练）
- 收敛速度更快
- 初始化质量更高

#### 2.2.3 性能表现

| 指标 | Qwen1.5-MoE-A2.7B | Qwen1.5-7B | Mistral-7B |
|------|-------------------|------------|------------|
| MMLU | 62.5 | 61.0 | 62.5 |
| GSM8K | 61.5 | 60.3 | 58.5 |
| HumanEval | 34.2 | 30.5 | 28.2 |
| MT-Bench | 7.17 | 7.07 | 7.03 |

**效率对比**：
- 训练成本：相比Qwen1.5-7B **降低75%**
- 推理速度：相比Qwen1.5-7B **提升1.74倍**

---

## Qwen2系列：全面升级

### 3.1 核心改进

Qwen2系列于2024年6月6日发布，带来多项重要升级：

| 改进方向 | 具体内容 |
|---------|---------|
| **多语言能力** | 支持中英日法等20+语言 |
| **长上下文** | 支持128K tokens上下文 |
| **推理能力** | 显著提升数学和代码能力 |
| **安全对齐** | 更好的安全性与有用性平衡 |

### 3.2 系列尺寸

| 模型 | 参数 | 上下文 | 特点 |
|------|------|--------|------|
| Qwen2-0.5B | 0.5B | 32K | 轻量部署 |
| Qwen2-1.5B | 1.5B | 32K | 平衡性能 |
| Qwen2-7B | 7B | 128K | 高性价比 |
| Qwen2-57B-A14B | 57B | 128K | MoE架构 |
| Qwen2-72B | 72B | 128K | 旗舰性能 |

### 3.3 技术创新

1. **分组查询注意力 (GQA)**：提升推理效率
2. **YARN上下文扩展**：**128K超长上下**文 rope的改良版
3. **增强Tokenizer**：151K词表，优化中文压缩率

---

## Qwen2.5系列：多样化与优化

### 4.1 发布时间

2024年9月19日

### 4.2 新增模型尺寸

```
Qwen2.5系列完整矩阵：
├── 0.5B    Qwen2.5-0.5B    边缘设备
├── 1.5B    Qwen2.5-1.5B    轻量部署
├── 3B      Qwen2.5-3B      【新增】移动端
├── 7B      Qwen2.5-7B      主力型号
├── 14B     Qwen2.5-14B     【新增】高性能
├── 32B     Qwen2.5-32B     【新增】本地大模型
└── 72B     Qwen2.5-72B     旗舰级别
```

### 4.3 核心改进

| 方面 | 提升 |
|------|------|
| **知识边界** | 截止到2024年6月 |
| **指令遵循** | 复杂指令理解增强 |
| **数学能力** | GSM8K/MATH显著提升 |
| **长文本** | 原生支持64K上下文 |
| **工具调用** | Function Call增强 |

---

## Qwen3系列：最新突破

### 5.1 发布时间

2025年4月29日（Qwen3-2507于2025年7-8月重大更新）

### 5.2 模型矩阵

#### 5.2.1 密集模型 (Dense)

| 模型 | 参数 | 特点 |
|------|------|------|
| Qwen3-0.6B | 0.6B | 极致轻量 |
| Qwen3-1.5B | 1.5B | 轻量高效 |
| Qwen3-4B | 4B | 性价比之选 |
| **Qwen3-8B** | **8B** | **主力型号** |
| Qwen3-14B | 14B | 高性能 |
| Qwen3-32B | 32B | 本地部署 |

#### 5.2.2 MoE模型

| 模型 | 激活参数 | 总参数量 |
|------|---------|---------|
| Qwen3-235B-A22B | 22B | 235B |
| Qwen3-30B-A3B | 3B | 30B |

#### 5.2.3 变体说明

```
每种模型有两种变体：
├── [模型名]        非思维模式 (Non-Thinking)
│   └── 适合：通用聊天、快速响应
│
└── [模型名]-Thinking  思维模式 (Thinking)
    └── 适合：复杂推理、数学计算
```

### 5.3 核心特性

#### 5.3.1 思考模式切换

```
┌─────────────────────────────────────────────────────────────────┐
│               思考模式切换机制                                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   非思考模式                      思考模式                       │
│   ┌─────────┐                    ┌─────────┐                 │
│   │ 快速响应 │  ─── /think ───►   │ 深度推理 │                 │
│   └─────────┘                    └─────────┘                 │
│        ▲                              │                        │
│        └────────── /no_think ────────┘                        │
│                                                                 │
│   示例：                                                        │
│   $ 你好                               $ 解这道数学题            │
│   你好！有什么帮...                    让我逐步思考...           │
│                     (输出思考过程)  →  最终答案是42             │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**采样参数对比**：

| 模式 | Temperature | Top-p | Top-k | Min-p |
|------|-------------|-------|-------|-------|
| 思考模式 | 0.6 | 0.95 | 20 | 0 |
| 非思考模式 | 0.7 | 0.8 | 20 | 0 |

**注意**：思考模式下**不要使用贪婪解码**，会导致性能下降。

#### 5.3.2 多语言支持

```
Qwen3支持100+语言和方言：

├── 中文 (Chinese)
├── 英语 (English)
├── 日语 (Japanese)
├── 法语 (French)
├── 西班牙语 (Spanish)
├── 阿拉伯语 (Arabic)
├── 俄语 (Russian)
├── 德语 (German)
├── 韩语 (Korean)
├── ... 等100+种语言
```

#### 5.3.3 长上下文 (YaRN)

| 模式 | 上下文长度 |
|------|-----------|
| 原生支持 | 32,768 tokens |
| YaRN扩展 | 131,072 tokens |
| 可扩展 | 1,000,000 tokens |

**vLLM部署配置**：
```bash
vllm serve Qwen/Qwen3-8B \
    --rope-scaling '{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}' \
    --max-model-len 131072
```

#### 5.3.4 工具调用与MCP协议

```python
from qwen_agent.agents import Assistant

tools = [
    {'mcpServers': {
        'time': {
            'command': 'uvx',
            'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']
        }
    }},
    'code_interpreter',
]

bot = Assistant(llm=llm_cfg, function_list=tools)
```

---

## Qwen3-8B 详细架构分析

### 6.1 基础参数

| 参数 | 值 |
|------|-----|
| **模型类型** | 稠密模型 (Dense) |
| **参数规模** | 8B (80亿) |
| **张量类型** | BF16 / FP16 / INT8 / INT4 |
| **上下文长度** | 32,768 tokens (原生) |
| **扩展上下文** | 131,072 tokens (YaRN) |

### 6.2 预估架构参数

基于Qwen系列架构演进规律，Qwen3-8B的架构参数预估如下：

```
┌─────────────────────────────────────────────────────────────────┐
│                    Qwen3-8B 架构概览                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Transformer Decoder only                                          │
│   ├── 层数 (num_layers):          32                            │
│   ├── 隐藏层大小 (hidden_size):  4096                           │
│   ├── 注意力头数 (num_heads):    32                             │
│   ├── 每头维度 (head_dim):       128                           │
│   ├── FFN中间维度:               14336                         │
│   └── 词表大小:                  151643                         │
│                                                                 │
│   注意力机制:                                               │
│   ├── 类型:                    Grouped Query Attention (GQA)   │
│   ├── KV头数:                 8 (4组，每组2个共享)              │
│   └── RoPE:                   YaRN动态缩放                     │
│                                                                 │
│   输出层:                                                    │
│   ├── 层归一化:                RMSNorm                         │
│   └── 激活函数:                SwiGLU                          │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 6.3 详细架构图

```
                          Qwen3-8B 完整前向传播

输入Token序列
      │
      ▼
┌─────────────────────────────────────────────────────────────────────┐
│                        Token Embedding                               │
│   (151643, 4096) → (batch, seq_len, 4096)                          │
└─────────────────────────────────────────────────────────────────────┘
      │
      ▼
┌─────────────────────────────────────────────────────────────────────┐
│                   RMSNorm + RoPE位置编码                             │
│   +──────────────────────────────────────────────────────────────+  │
│   │ LayerNorm (4096) + YaRN RoPE                                  │  │
│   +──────────────────────────────────────────────────────────────+  │
└─────────────────────────────────────────────────────────────────────┘
      │
      ▼
┌─────────────────────────────────────────────────────────────────────┐
│                         Transformer Block × 32                       │
│   ┌─────────────────────────────────────────────────────────────┐   │
│   │  注意力层                                                     │   │
│   │  ├── Q_proj: (4096, 4096)                                    │   │
│   │  ├── K_proj: (4096, 1024)  [GQA, 8 heads]                    │   │
│   │  ├── V_proj: (4096, 1024)  [GQA, 8 heads]                    │   │
│   │  ├── O_proj: (4096, 4096)                                    │   │
│   │  └── Grouped Query Attention (num_heads=32, num_kv_heads=8)  │   │
│   └─────────────────────────────────────────────────────────────┘   │
│                              │                                      │
│                              ▼                                      │
│   ┌─────────────────────────────────────────────────────────────┐   │
│   │  前馈网络 (FFN)                                              │   │
│   │  ├── Gate_proj: (4096, 14336)                               │   │
│   │  ├── Up_proj:   (4096, 14336)                               │   │
│   │  ├── Down_proj: (14336, 4096)                               │   │
│   │  └── SwiGLU 激活函数                                         │   │
│   └─────────────────────────────────────────────────────────────┘   │
│                              │                                      │
│                              ▼                                      │
│   ┌─────────────────────────────────────────────────────────────┐   │
│   │  残差连接 + RMSNorm                                         │   │
│   └─────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────┘
      │
      ▼
┌─────────────────────────────────────────────────────────────────────┐
│                      最终 Layer Norm                                 │
│   RMSNorm (4096)                                                    │
└─────────────────────────────────────────────────────────────────────┘
      │
      ▼
┌─────────────────────────────────────────────────────────────────────┐
│                         LM Head                                      │
│   Linear (4096, 151643) → logits                                    │
└─────────────────────────────────────────────────────────────────────┘
      │
      ▼
┌─────────────────────────────────────────────────────────────────────┐
│                      Softmax → 概率分布                              │
│   (batch, seq_len, 151643)                                          │
└─────────────────────────────────────────────────────────────────────┘
```

### 6.4 计算复杂度分析

| 指标 | 计算公式 | 数值估算 |
|------|---------|---------|
| **注意力计算量** | `4 × N × d² × num_heads` | ~8.6 TFLOPs |
| **FFN计算量** | `24 × N × d²` | ~51.8 TFLOPs |
| **总计算量/步** | - | ~60 TFLOPs |
| **参数量** | - | 8B |
| **显存占用 (FP16)** | 2 bytes × 8B | ~16GB |

### 6.5 与前代对比

| 特性 | Qwen2-7B | Qwen2.5-7B | **Qwen3-8B** |
|------|----------|------------|--------------|
| 层数 | 28 | 28 | **32** |
| 隐藏层大小 | 4096 | 4096 | 4096 |
| 注意力头数 | 32 | 32 | 32 |
| GQA | 否 | 是 | **是** |
| 上下文长度 | 128K | 128K | 131K |
| 思考模式 | 否 | 否 | **是** |
| 工具调用 | 基础 | 增强 | **MCP完整** |

---

## 技术演进总结

### 7.1 架构演进

```
Qwen1 ──────► Qwen1.5 ──────► Qwen2 ──────► Qwen2.5 ──────► Qwen3
  │             │               │              │               │
  ▼             ▼               ▼              ▼               ▼
标准Transformer  首个MoE         GQA优化       尺寸多样化      思考模式
  │         Upcycling         长上下文        长文本优化      MCP工具
  │             │               │              │               │
  ▼             ▼               ▼              ▼               ▼
7B/14B/70B  2.7B激活       128K上下文     0.5B-72B       0.6B-235B
```

### 7.2 关键技术突破

| 版本 | 突破点 |
|------|--------|
| Qwen1.5-MoE | Upcycling训练、细粒度专家 |
| Qwen2 | 多语言支持、GQA、128K上下文 |
| Qwen2.5 | 3B/14B/32B新尺寸、工具调用 |
| Qwen3 | 思考模式切换、100+语言、235B MoE |

### 7.3 性能提升趋势

```
性能指标对比 (7B级别模型)

          MMLU   GSM8K  HumanEval  MT-Bench
Qwen1.5-7B  61.0   60.3    30.5       7.07
Qwen2-7B    64.6   62.5    37.1       7.60
Qwen2.5-7B  70.5   78.5    48.5       8.10
Qwen3-8B    74.1   82.3    54.3       8.40
                     ▲ 持续提升中
```

---

## 参考文献

1. **Qwen3 Technical Report** - [arXiv:2505.09388](https://arxiv.org/abs/2505.09388)
2. **Qwen MoE Blog** - [qwenlm.github.io/blog/qwen-moe](https://qwenlm.github.io/blog/qwen-moe/)
3. **Qwen3 HF Model Card** - [huggingface.co/Qwen/Qwen3-8B](https://huggingface.co/Qwen/Qwen3-8B)
4. **Qwen GitHub** - [github.com/QwenLM/Qwen3](https://github.com/QwenLM/Qwen3)

---

## 附录1：专业术语解释

### A. 模型架构相关

| 术语 | 英文全称 | 解释 |
|------|---------|------|
| **Transformer** | - | 2017年提出的基于自注意力机制的神经网络架构，是现代大语言模型的基础 |
| **Decoder-only** | - | 仅使用Transformer解码器部分的架构，**GPT系列采用此架构**，此外还有encoder-decoder，encoder-only |
| **Dense Model** | - | 稠密模型，所有参数在每次推理时都参与计算 |
| **MoE** | Mixture of Experts | 混合专家模型，推理的时候只有部分专家参数被激活，大幅降低计算成本 |
| **Top-K Routing** | - | 路由策略，选择得分最高的K个专家参与计算 |
| **Upcycling** | - | 将已有稠密模型转换为MoE模型的训练策略 |

### B. 注意力机制相关

| 术语 | 英文全称 | 解释 |
|------|---------|------|
| **Self-Attention** | 自注意力机制 | Transformer核心组件，让序列中每个位置都能关注其他所有位置 |
| **MHA** | Multi-Head Attention | 多头注意力，并行执行多个注意力头，捕获不同子空间特征 |
| **GQA** | Grouped Query Attention | 分组查询注意力，Q和KV分组共享，减少KV缓存 |
| **KV Cache** | - | 键值缓存，推理时缓存已计算的K和V，节省重复计算 |
| **RoPE** | Rotary Position Embedding | 旋转位置编码，相对位置编码的一种，**支持长上下文** |

### C. 训练与优化相关

| 术语 | 英文全称 | 解释 |
|------|---------|------|
| **Pre-training** | 预训练 | 在大规模无标注数据上训练模型，学习语言知识 |
| **SFT** | Supervised Fine-Tuning | 监督微调，使用**标注数据**进行指令微调 |
| **LoRA** | Low-Rank Adaptation | 低秩适配，**只训练低秩矩阵**，大幅降低微调成本 |
| **RLHF** | Reinforcement Learning from Human Feedback | 基于人类反馈的强化学习，用于模型对齐，sft-reward model - ppo/grpo，早期缺点是不太稳定 |
| **RMSNorm** | Root Mean Square Layer Normalization | 均方根归一化，比LayerNorm更高效的归一化方法 |

### D. 激活函数相关

| 术语 | 英文全称 | 解释 |
|------|---------|------|
| **SwiGLU** | Swish-Gated Linear Unit | 门控线性单元，结合Swish激活和门控机制 |
| **Swish** | - | x × sigmoid(βx)，Google提出的激活函数 |
| **GeLU** | Gaussian Error Linear Unit | 高斯误差线性单元 |

### E. 推理相关

| 术语 | 英文全称 | 解释 |
|------|---------|------|
| **Temperature** | - | 温度参数，控制采样随机性，值越低越确定 |
| **Top-p** | Nucleus Sampling | 核采样，只从累积概率超过p的词中采样 |
| **Top-k** | - | 只考虑概率最高的k个词 |
| **Greedy Decoding** | 贪婪解码 | 每次选择概率最高的词 |
| **Beam Search** | 束搜索 | 维护多条候选路径，综合选择最优解 |
| **YaRN** | Yet another RoPE extension | **RoPE扩展方法，用于延长上下文窗口** |



token

1 0.8 2 0.15 3 0.05

temperature 上升之后，模型输出越不稳定，随机性越大，创造力越强

1 0.5 2 0.3 3 0.2



### F. 评估指标相关

| 术语 | 英文全称 | 解释 |
|------|---------|------|
| **MMLU** | Massive Multitask Language Understanding | 大规模多任务语言理解，评估知识和推理能力 |
| **GSM8K** | - | 小学数学数据集，包含8.5K道数学题 |
| **HumanEval** | - | 代码生成评估数据集 |
| **MT-Bench** | - | 多轮对话评估基准 |
| **F1-Score** | - | 精确率和召回率的调和平均 |

### G. 部署相关

| 术语 | 英文全称 | 解释 |
|------|---------|------|
| **vLLM** | - | 高效的LLM推理服务框架，支持PagedAttention |
| **BF16** | Brain Float 16 | 16位浮点格式，指数范围与FP32相同 |
| **FP16** | Half Precision | 16位浮点 |
| **INT8/INT4** | - | 8位/4位整数量化，减小模型体积和加速推理 |
| **Quantization** | 量化 | 将浮点参数转换为低精度格式 |

### H. 其他专业术语

| 术语 | 解释 |
|------|------|
| **Token** | 文本切分的最小单位，通常是子词或词 |
| **Context Length** | 上下文长度，模型单次能处理的最大token数 |
| **Tokenizer** | 分词器，将文本转换为token序列 |
| **Vocabulary Size** | 词表大小，tokenizer包含的唯一token数量 |
| **Chain-of-Thought** | 思维链，让模型逐步推理提升复杂任务表现 |
| **Few-shot** | 少样本，给模型几个示例帮助理解任务 |
| **Zero-shot** | 零样本，不给示例直接推理 |
| **MCP** | Model Context Protocol | 模型上下文协议，用于工具调用 |
| **Function Call** | 函数调用 | 模型调用外部工具的能力 |

---

## 附录2：Qwen3-8B 快速使用

### 安装

```bash
pip install transformers accelerate
```

### 推理代码

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-8B",
    torch_dtype="auto",
    device_map="auto",
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-8B")

prompt = "请介绍一下你自己。"
messages = [
    {"role": "system", "content": "你是一个有帮助的助手。"},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer([text], return_tensors="pt").to(model.device)

outputs = model.generate(**inputs, max_new_tokens=32768, temperature=0.6)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

### 思考模式

```python
# 启用思考模式
messages = [
    {"role": "system", "content": "你是一个有帮助的助手。"},
    {"role": "user", "content": "请逐步思考这个问题：鸡兔同笼..."},
]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=True)
```

---
