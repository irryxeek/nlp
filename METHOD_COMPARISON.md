# 电商评论观点挖掘：LLM方法 vs Baseline方法对比分析

## 1. 任务概述

**任务**：从电商评论中抽取观点四元组
```
(AspectTerm, OpinionTerm, Polarity, Category)
```
- 属性特征词、观点词、情感极性、属性种类

---

## 2. 方法对比

### 2.1 Baseline方法（OpinioNet）

基于 BERT/RoBERTa/ERNIE 的 **One-stage 端到端** 实体关系抽取模型。

**技术特点**：
- 多任务联合学习：aspect抽取、opinion抽取、情感分类、属性分类
- 指针网络（Pointer Network）进行序列标注
- 复杂的置信度计算与beam search解码
- 需要预训练 + 领域适应 + 多模型集成

**模型复杂度**：
```
BERT编码器 → 多层指针网络 → 7个输出头
├── as_logits (Aspect起始)
├── ae_logits (Aspect结束)
├── os_logits (Opinion起始)
├── oe_logits (Opinion结束)
├── obj_logits (目标判定)
├── c_logits (Category分类)
└── p_logits (Polarity分类)
```

**训练流程**：
1. 无监督预训练（MLM + 下游任务）
2. 领域适应微调
3. 交叉验证训练
4. 多模型集成（RoBERTa + WWM + ERNIE）

---

### 2.2 LLM方法（我们的方法）

基于 **大语言模型** 的观点抽取方法。

**技术路线**：

1. **零样本/少样本抽取**：直接使用 Gemini 2.5 Flash / gpt 5.1（**可能出现四元祖数量提取不完整的现象**，可以考虑加入few-shot prompt）
2. **有监督微调**：LLaMA 3.1 8B + **LoRA**

**核心代码**：

```python
# Gemini API 调用 可以考虑加入rag对category进行抽取
SYSTEM_PROMPT = """你是一个电商评论分析助手。
对于给定的中文商品评论，抽取其中所有的观点四元组：
{属性特征词 AspectTerm, 观点词 OpinionTerm, 情感极性 Polarity, 属性种类 Category}。
要求：
1. AspectTerm 和 OpinionTerm 必须从原始评论中逐字拷贝，不能改写。
2. Polarity 只能取"正面""负面""中性"三类。
3. Category 必须从给定集合中选择（训练集里面出现过的category。
4. 最终输出必须是 JSON 对象格式。
"""
```

---

## 3. LLM方法的优势分析

### 3.1 语义理解能力

| 方面 | Baseline | LLM方法 |
|------|----------|---------|
| 语义理解 | 依赖预训练BERT的浅层表示 | 深层语义理解 |
| 上下文理解 | 序列标注，依赖局部特征 | 强上下文推理能力 |
| 中文理解 | 需要领域适应 | 原生支持中文 |

**案例分析**：
```
评论："包装太随便了，连个包装盒都没有"

Baseline：需要学习"包装"→"包装"、"随便"→负面等映射
LLM：直接理解语义"包装"是属性，"太随便了"是负面观点
```

### 3.2 任务适配成本

| 阶段 | Baseline | LLM方法 |
|------|----------|---------|
| 模型开发 | 需要设计复杂网络结构 | 只需编写Prompt |
| 训练数据 | 需要大量标注数据 | 可零样本，可少样本 |
| 领域适应 | 预训练+微调多阶段 | 少量领域数据即可 |
| 工程实现 | 指针网络+beam search | API调用或简单微调 |

**时间成本对比**：
- Baseline：约 24小时（预训练+微调+集成）
- LLM方法：分钟级（零样本）或 小时级（微调）

### 3.3 泛化能力

```
┌─────────────────────────────────────────────────────────────┐
│  LLM方法的优势场景                                           │
├─────────────────────────────────────────────────────────────┤
│  1. 新领域快速适配                                           │
│     只需修改Prompt，无需重新训练模型                          │
│                                                             │
│  2. 多任务统一框架                                           │
│     同一个LLM可处理抽取、分类、生成等多种任务                 │
│                                                             │
│  3. 复杂表达理解                                            │
│     隐式观点、讽刺、反语等自然语言现象                       │
│                                                             │
│  4. 零样本学习                                              │
│     未见过的属性类别也能通过语义理解进行推理                  │
└─────────────────────────────────────────────────────────────┘
```

### 3.4 结构化输出能力

**Baseline的解码流程**：

```
BERT输出 → 指针网络 → 7个logits → 置信度计算 → NMS去重 → 规则过滤 → 输出
```
涉及复杂的阈值调参、beam search、约束规则。

**LLM的结构化输出**：
```
输入文本 + System Prompt → LLM → JSON格式输出
```
通过 **Prompt Engineering** 直接指定输出格式：

```json
{"tuples": [{"aspect": "...","opinion": "...","polarity": "...","category": "..."}]}
```


## 4. 技术路线总结

```
┌─────────────────────────────────────────────────────────────────┐
│                    LLM方法技术栈                                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   ┌─────────────┐     ┌─────────────┐     ┌─────────────┐      │
│   │ Gemini API  │  →  │ Prompt Eng  │  →  │ JSON 解析   │      │
│   │ (零样本)    │     │             │     │             │      │
│   └─────────────┘     └─────────────┘     └─────────────┘      │
│         │                   │                   │               │
│         ▼                   ▼                   ▼               │
│   无需训练            结构化输出         易于后处理             │
│                                                                 │
│   ┌─────────────┐     ┌─────────────┐     ┌─────────────┐      │
│   │ LLaMA 3.1   │  →  │ LoRA 微调   │  →  │ 推理部署    │      │
│   │ 8B Instruct │     │ 高效适配    │     │             │      │
│   └─────────────┘     └─────────────┘     └─────────────┘      │
│         │                   │                   │               │
│         ▼                   ▼                   ▼               │
│   指令遵循            低成本训练          本地部署             │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```



## 7. 代码文件说明

| 文件 | 功能 |
|------|------|
| `gemini_extraction.py` | Gemini API 零样本抽取 |
| `build_sft_dataset.py` | 构建SFT训练数据 supervised finetune training |
| `finetune_lora.py` | **LLaMA LoRA 微调（重点在于理解lora的算法**而非代码） |
| `merge_lora_model.py` | 合并LoRA权重 |
| `fix_result_format.py` | 修正输出格式，人工清洗数据 |
